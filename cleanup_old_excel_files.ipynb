{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Unity Catalog Volume - Excel File Cleanup\n\nThis notebook deletes Excel files (`.xlsx`, `.xls`) older than a configurable retention period from Unity Catalog Volume paths.\n\n**Scheduled**: Daily via Databricks Workflow  \n**Configuration**: All parameters are exposed as Databricks widgets for job-level overrides.\n\n### Enterprise Features\n- **Structured logging** with Python `logging` (timestamped, leveled)\n- **Input validation** with early-fail on misconfiguration\n- **Retry logic** with configurable attempts for transient deletion failures\n- **Idempotency guards** - verifies file existence before and after operations\n- **Per-volume statistics** for granular observability\n- **Execution timing** for performance monitoring\n- **Hard failure on errors** - raises exception to fail the Databricks job/workflow"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration - Widget Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ---------------------------------------------------------------\n# Widget definitions - override these at job/task level as needed\n# ---------------------------------------------------------------\n\ndbutils.widgets.text(\n    \"volume_paths\",\n    \"/Volumes/catalog1/schema1/volume1,/Volumes/catalog1/schema1/volume2\",\n    \"Comma-separated volume paths to scan\",\n)\n\ndbutils.widgets.text(\n    \"retention_days\",\n    \"29\",\n    \"Delete files older than this many days\",\n)\n\ndbutils.widgets.text(\n    \"file_extensions\",\n    \".xlsx,.xls\",\n    \"Comma-separated file extensions to target\",\n)\n\ndbutils.widgets.dropdown(\n    \"dry_run\",\n    \"true\",\n    [\"true\", \"false\"],\n    \"Dry run mode (true = list only, false = delete)\",\n)\n\ndbutils.widgets.text(\n    \"max_retries\",\n    \"3\",\n    \"Max retry attempts for transient file-deletion failures\",\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Logging Setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import logging\nimport os\nimport sys\nimport time\nfrom datetime import datetime, timezone\n\n# ---------------------------------------------------------------------------\n# Configure structured logger (avoids duplicate handlers on re-runs)\n# ---------------------------------------------------------------------------\nLOG_FORMAT = \"%(asctime)s | %(levelname)-8s | %(message)s\"\nDATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\nlogger = logging.getLogger(\"excel_cleanup\")\nlogger.setLevel(logging.INFO)\nlogger.handlers.clear()\n\n_handler = logging.StreamHandler(sys.stdout)\n_handler.setFormatter(logging.Formatter(LOG_FORMAT, datefmt=DATE_FORMAT))\nlogger.addHandler(_handler)\n\nlogger.info(\"Logger initialised\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Parse & Validate Widget Values"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ---------------------------------------------------------------------------\n# Parse widgets into typed config\n# ---------------------------------------------------------------------------\nVOLUME_PATHS = [\n    p.strip() for p in dbutils.widgets.get(\"volume_paths\").split(\",\") if p.strip()\n]\nRETENTION_DAYS = int(dbutils.widgets.get(\"retention_days\"))\nFILE_EXTENSIONS = tuple(\n    ext.strip().lower()\n    for ext in dbutils.widgets.get(\"file_extensions\").split(\",\")\n    if ext.strip()\n)\nDRY_RUN = dbutils.widgets.get(\"dry_run\").lower() == \"true\"\nMAX_RETRIES = int(dbutils.widgets.get(\"max_retries\"))\n\n# Pre-compute the cutoff timestamp (seconds since epoch)\nCUTOFF_EPOCH = time.time() - (RETENTION_DAYS * 86400)\nCUTOFF_DATE = datetime.fromtimestamp(CUTOFF_EPOCH, tz=timezone.utc)\n\n# ---------------------------------------------------------------------------\n# Input validation - fail fast on misconfiguration\n# ---------------------------------------------------------------------------\n_errors = []\nif not VOLUME_PATHS:\n    _errors.append(\"volume_paths is empty - at least one path is required\")\nfor _p in VOLUME_PATHS:\n    if not _p.startswith(\"/Volumes/\"):\n        _errors.append(f\"Invalid volume path (must start with /Volumes/): {_p}\")\nif RETENTION_DAYS < 1:\n    _errors.append(f\"retention_days must be >= 1, got {RETENTION_DAYS}\")\nif not FILE_EXTENSIONS:\n    _errors.append(\"file_extensions is empty - at least one extension is required\")\nfor _ext in FILE_EXTENSIONS:\n    if not _ext.startswith(\".\"):\n        _errors.append(f\"Extension must start with a dot: '{_ext}'\")\nif MAX_RETRIES < 0:\n    _errors.append(f\"max_retries must be >= 0, got {MAX_RETRIES}\")\n\nif _errors:\n    for _e in _errors:\n        logger.error(f\"VALIDATION FAILURE: {_e}\")\n    raise ValueError(\n        f\"Configuration validation failed with {len(_errors)} error(s). \"\n        \"Check logs above for details.\"\n    )\n\n# ---------------------------------------------------------------------------\n# Log validated configuration\n# ---------------------------------------------------------------------------\nlogger.info(\"=\" * 60)\nlogger.info(\"CONFIGURATION (validated)\")\nlogger.info(\"=\" * 60)\nlogger.info(f\"Volume paths     : {VOLUME_PATHS}\")\nlogger.info(f\"Retention days   : {RETENTION_DAYS}\")\nlogger.info(f\"File extensions  : {FILE_EXTENSIONS}\")\nlogger.info(f\"Dry run          : {DRY_RUN}\")\nlogger.info(f\"Max retries      : {MAX_RETRIES}\")\nlogger.info(f\"Cutoff date (UTC): {CUTOFF_DATE:%Y-%m-%d %H:%M:%S}\")\nlogger.info(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Helper Functions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def format_bytes(size_bytes: int) -> str:\n    \"\"\"Return a human-readable file-size string.\"\"\"\n    for unit in (\"B\", \"KB\", \"MB\", \"GB\", \"TB\"):\n        if abs(size_bytes) < 1024:\n            return f\"{size_bytes:.1f} {unit}\"\n        size_bytes /= 1024\n    return f\"{size_bytes:.1f} PB\"\n\n\ndef format_duration(seconds: float) -> str:\n    \"\"\"Return a human-readable duration string.\"\"\"\n    if seconds < 60:\n        return f\"{seconds:.1f}s\"\n    minutes, secs = divmod(seconds, 60)\n    return f\"{int(minutes)}m {secs:.1f}s\"\n\n\ndef scan_files(base_path: str, extensions: tuple[str, ...]) -> list[str]:\n    \"\"\"Recursively find all files matching the target extensions under *base_path*.\"\"\"\n    matched = []\n    try:\n        for root, _dirs, files in os.walk(base_path):\n            for fname in files:\n                if fname.lower().endswith(extensions):\n                    matched.append(os.path.join(root, fname))\n    except PermissionError as exc:\n        logger.warning(f\"Permission denied while scanning {base_path}: {exc}\")\n    except OSError as exc:\n        logger.error(f\"OS error scanning {base_path}: {exc}\")\n    return matched\n\n\ndef is_older_than_cutoff(file_path: str, cutoff_epoch: float) -> bool:\n    \"\"\"Return True if the file's modification time is before the cutoff.\"\"\"\n    return os.path.getmtime(file_path) < cutoff_epoch\n\n\ndef delete_file_with_retry(\n    file_path: str,\n    dry_run: bool,\n    max_retries: int,\n) -> dict:\n    \"\"\"Delete a single file with retry logic. Returns a result dict.\n\n    Retries on OSError/PermissionError up to *max_retries* times with\n    exponential back-off (1s, 2s, 4s ...).  PermissionError on all\n    attempts is treated as a hard error.\n    \"\"\"\n    mtime = os.path.getmtime(file_path)\n    size_bytes = os.path.getsize(file_path)\n    age_days = (time.time() - mtime) / 86400\n\n    result = {\n        \"file_path\": file_path,\n        \"size_bytes\": size_bytes,\n        \"age_days\": round(age_days, 1),\n        \"modified_utc\": datetime.fromtimestamp(mtime, tz=timezone.utc).strftime(\n            \"%Y-%m-%d %H:%M:%S\"\n        ),\n        \"status\": \"pending\",\n        \"attempts\": 0,\n    }\n\n    if dry_run:\n        result[\"status\"] = \"dry_run_skipped\"\n        return result\n\n    # Idempotency guard - file may have been removed between scan and delete\n    if not os.path.exists(file_path):\n        result[\"status\"] = \"already_removed\"\n        logger.info(f\"File already removed (idempotent skip): {file_path}\")\n        return result\n\n    last_exc = None\n    for attempt in range(1, max_retries + 1):\n        result[\"attempts\"] = attempt\n        try:\n            os.remove(file_path)\n            # Verify deletion\n            if not os.path.exists(file_path):\n                result[\"status\"] = \"deleted\"\n                return result\n            else:\n                logger.warning(\n                    f\"os.remove returned but file still exists: {file_path}\"\n                )\n                result[\"status\"] = \"error: file still exists after os.remove\"\n                return result\n        except FileNotFoundError:\n            # Another process deleted it between our check and os.remove\n            result[\"status\"] = \"already_removed\"\n            return result\n        except (PermissionError, OSError) as exc:\n            last_exc = exc\n            if attempt < max_retries:\n                backoff = 2 ** (attempt - 1)\n                logger.warning(\n                    f\"Attempt {attempt}/{max_retries} failed for {file_path}: \"\n                    f\"{exc} - retrying in {backoff}s\"\n                )\n                time.sleep(backoff)\n\n    result[\"status\"] = f\"error: {last_exc}\"\n    return result"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Scan and Delete Old Excel Files"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "RUN_START = time.time()\n\nall_results: list[dict] = []\nper_volume_stats: dict[str, dict] = {}  # per-path breakdown\n\nglobal_summary = {\n    \"paths_configured\": len(VOLUME_PATHS),\n    \"paths_scanned\": 0,\n    \"paths_skipped\": 0,\n    \"files_scanned\": 0,\n    \"files_eligible\": 0,\n    \"files_retained\": 0,\n    \"files_deleted\": 0,\n    \"files_already_removed\": 0,\n    \"errors\": 0,\n    \"total_bytes_freed\": 0,\n}\n\nfor vol_path in VOLUME_PATHS:\n    logger.info(f\"Scanning: {vol_path}\")\n\n    vol_stats = {\n        \"files_scanned\": 0,\n        \"files_eligible\": 0,\n        \"files_retained\": 0,\n        \"files_deleted\": 0,\n        \"errors\": 0,\n        \"bytes_freed\": 0,\n    }\n\n    if not os.path.isdir(vol_path):\n        logger.warning(f\"Path does not exist or is not a directory - skipping: {vol_path}\")\n        global_summary[\"paths_skipped\"] += 1\n        per_volume_stats[vol_path] = vol_stats\n        continue\n\n    global_summary[\"paths_scanned\"] += 1\n    excel_files = scan_files(vol_path, FILE_EXTENSIONS)\n    vol_stats[\"files_scanned\"] = len(excel_files)\n    global_summary[\"files_scanned\"] += len(excel_files)\n    logger.info(f\"  Found {len(excel_files)} matching file(s)\")\n\n    for fpath in excel_files:\n        try:\n            if not is_older_than_cutoff(fpath, CUTOFF_EPOCH):\n                vol_stats[\"files_retained\"] += 1\n                global_summary[\"files_retained\"] += 1\n                continue\n        except OSError as exc:\n            logger.error(f\"  Cannot stat file {fpath}: {exc}\")\n            vol_stats[\"errors\"] += 1\n            global_summary[\"errors\"] += 1\n            all_results.append({\"file_path\": fpath, \"status\": f\"error: {exc}\"})\n            continue\n\n        global_summary[\"files_eligible\"] += 1\n        vol_stats[\"files_eligible\"] += 1\n\n        result = delete_file_with_retry(fpath, DRY_RUN, MAX_RETRIES)\n        all_results.append(result)\n\n        if result[\"status\"] == \"deleted\":\n            vol_stats[\"files_deleted\"] += 1\n            vol_stats[\"bytes_freed\"] += result[\"size_bytes\"]\n            global_summary[\"files_deleted\"] += 1\n            global_summary[\"total_bytes_freed\"] += result[\"size_bytes\"]\n        elif result[\"status\"] == \"already_removed\":\n            global_summary[\"files_already_removed\"] += 1\n        elif result[\"status\"].startswith(\"error\"):\n            vol_stats[\"errors\"] += 1\n            global_summary[\"errors\"] += 1\n\n        level = logging.WARNING if result[\"status\"].startswith(\"error\") else logging.INFO\n        logger.log(\n            level,\n            f\"  [{result['status'].upper()}] {result['file_path']} \"\n            f\"| age={result.get('age_days', '?')}d \"\n            f\"| size={format_bytes(result.get('size_bytes', 0))} \"\n            f\"| modified={result.get('modified_utc', 'N/A')}\"\n            + (f\" | attempts={result.get('attempts')}\" if result.get(\"attempts\", 0) > 1 else \"\"),\n        )\n\n    per_volume_stats[vol_path] = vol_stats\n    logger.info(\n        f\"  Volume summary: scanned={vol_stats['files_scanned']} \"\n        f\"eligible={vol_stats['files_eligible']} \"\n        f\"deleted={vol_stats['files_deleted']} \"\n        f\"retained={vol_stats['files_retained']} \"\n        f\"errors={vol_stats['errors']} \"\n        f\"freed={format_bytes(vol_stats['bytes_freed'])}\"\n    )\n\nRUN_DURATION = time.time() - RUN_START"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Summary Report"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "logger.info(\"\")\nlogger.info(\"=\" * 60)\nlogger.info(\"EXECUTION SUMMARY\")\nlogger.info(\"=\" * 60)\nlogger.info(f\"Mode                 : {'DRY RUN' if DRY_RUN else 'LIVE DELETE'}\")\nlogger.info(f\"Run duration         : {format_duration(RUN_DURATION)}\")\nlogger.info(f\"Volume paths config  : {global_summary['paths_configured']}\")\nlogger.info(f\"Volume paths scanned : {global_summary['paths_scanned']}\")\nlogger.info(f\"Volume paths skipped : {global_summary['paths_skipped']}\")\nlogger.info(f\"Total files found    : {global_summary['files_scanned']}\")\nlogger.info(f\"Files retained (<{RETENTION_DAYS}d): {global_summary['files_retained']}\")\nlogger.info(f\"Eligible for delete  : {global_summary['files_eligible']} (older than {RETENTION_DAYS} days)\")\n\nif DRY_RUN:\n    logger.info(f\"Files deleted        : 0 (dry run - no files removed)\")\nelse:\n    logger.info(f\"Files deleted        : {global_summary['files_deleted']}\")\n    logger.info(f\"Already removed      : {global_summary['files_already_removed']}\")\n    logger.info(f\"Space freed          : {format_bytes(global_summary['total_bytes_freed'])}\")\n\nlogger.info(f\"Errors               : {global_summary['errors']}\")\nlogger.info(\"=\" * 60)\n\n# Per-volume breakdown\nlogger.info(\"\")\nlogger.info(\"PER-VOLUME BREAKDOWN\")\nlogger.info(\"-\" * 60)\nfor vp, vs in per_volume_stats.items():\n    logger.info(\n        f\"  {vp}: scanned={vs['files_scanned']} eligible={vs['files_eligible']} \"\n        f\"deleted={vs['files_deleted']} retained={vs['files_retained']} \"\n        f\"errors={vs['errors']} freed={format_bytes(vs['bytes_freed'])}\"\n    )\nlogger.info(\"-\" * 60)\n\n# Error details\nif global_summary[\"errors\"] > 0:\n    logger.error(\"\")\n    logger.error(\"ERROR DETAILS:\")\n    for r in all_results:\n        if r[\"status\"].startswith(\"error\"):\n            logger.error(f\"  {r['file_path']} -> {r['status']}\")\n\nif DRY_RUN and global_summary[\"files_eligible\"] > 0:\n    logger.info(\n        f\"\\nNOTE: Set dry_run=false to actually delete the \"\n        f\"{global_summary['files_eligible']} eligible file(s).\"\n    )"
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Exit - Raise on Errors to Fail the Databricks Job",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ---------------------------------------------------------------------------\n# Exit strategy:\n#   - On ANY errors  -> raise RuntimeError so the Databricks workflow marks\n#     the run as FAILED, triggering built-in alerting.\n#   - On success     -> exit cleanly with a status message.\n# ---------------------------------------------------------------------------\nmode_label = \"DRY_RUN\" if DRY_RUN else \"LIVE\"\n\nexit_msg = (\n    f\"{mode_label}: \"\n    f\"scanned={global_summary['files_scanned']} \"\n    f\"eligible={global_summary['files_eligible']} \"\n    f\"deleted={global_summary['files_deleted']} \"\n    f\"errors={global_summary['errors']} \"\n    f\"freed={format_bytes(global_summary['total_bytes_freed'])} \"\n    f\"duration={format_duration(RUN_DURATION)}\"\n)\n\nif global_summary[\"errors\"] > 0:\n    logger.error(f\"Job will FAIL due to {global_summary['errors']} error(s)\")\n    dbutils.notebook.exit(f\"FAILED: {exit_msg}\")\n    raise RuntimeError(\n        f\"Excel cleanup completed with {global_summary['errors']} error(s). \"\n        f\"Details: {exit_msg}\"\n    )\nelse:\n    logger.info(f\"Job completed successfully: {exit_msg}\")\n    dbutils.notebook.exit(f\"SUCCESS: {exit_msg}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}