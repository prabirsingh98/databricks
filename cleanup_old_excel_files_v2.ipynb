{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fa761fa-a992-4432-8028-091ee4656b1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Unity Catalog Volume - Excel File Cleanup\n",
    "\n",
    "This notebook deletes Excel files (`.xlsx`, `.xls`) older than a configurable retention period from Unity Catalog Volume paths.\n",
    "\n",
    "**Scheduled**: Daily via Databricks Workflow  \n",
    "**Configuration**: All parameters are exposed as Databricks widgets for job-level overrides.\n",
    "\n",
    "### Enterprise Features\n",
    "- **Structured logging** with Python `logging` (timestamped, leveled)\n",
    "- **Input validation** with early-fail on misconfiguration\n",
    "- **Retry logic** with configurable attempts for transient deletion failures\n",
    "- **Idempotency guards** - verifies file existence before and after operations\n",
    "- **Per-volume statistics** for granular observability\n",
    "- **Execution timing** for performance monitoring\n",
    "- **Hard failure on errors** - raises exception to fail the Databricks job/workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7854c49-d8b4-400d-8e03-867f45e9c52d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Configuration - Widget Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "920af171-bd3b-498c-9b58-071f97de6aa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Widget definitions - override these at job/task level as needed\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "dbutils.widgets.text(\n",
    "    \"volume_paths\",\n",
    "    \"/Volumes/accenture/prasun12377/claims_checkpoint_dir,/Volumes/accenture/prasun12377/bcbsm_volume\",\n",
    "    \"Comma-separated volume paths to scan\",\n",
    ")\n",
    "\n",
    "dbutils.widgets.text(\n",
    "    \"retention_days\",\n",
    "    \"0\",\n",
    "    \"Delete files older than this many days\",\n",
    ")\n",
    "\n",
    "dbutils.widgets.text(\n",
    "    \"file_extensions\",\n",
    "    \".xlsx,.xls\",\n",
    "    \"Comma-separated file extensions to target\",\n",
    ")\n",
    "\n",
    "dbutils.widgets.dropdown(\n",
    "    \"dry_run\",\n",
    "    \"true\",\n",
    "    [\"true\", \"false\"],\n",
    "    \"Dry run mode (true = list only, false = delete)\",\n",
    ")\n",
    "\n",
    "dbutils.widgets.text(\n",
    "    \"max_retries\",\n",
    "    \"3\",\n",
    "    \"Max retry attempts for transient file-deletion failures\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c6d442f-68c0-4910-b300-04640e916c68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be1cd3ab-f08d-4e4e-9ad3-863b86ef88bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-24 05:43:55 | INFO     | Logger initialised\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Configure structured logger (avoids duplicate handlers on re-runs)\n",
    "# ---------------------------------------------------------------------------\n",
    "LOG_FORMAT = \"%(asctime)s | %(levelname)-8s | %(message)s\"\n",
    "DATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "logger = logging.getLogger(\"excel_cleanup\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers.clear()\n",
    "\n",
    "_handler = logging.StreamHandler(sys.stdout)\n",
    "_handler.setFormatter(logging.Formatter(LOG_FORMAT, datefmt=DATE_FORMAT))\n",
    "logger.addHandler(_handler)\n",
    "\n",
    "logger.info(\"Logger initialised\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95286b75-605a-46a9-aa2a-df71a89fad49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Parse & Validate Widget Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be9dba8e-6572-4134-a5fa-e72dbf9d3ea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-24 05:43:56 | INFO     | ============================================================\n2026-02-24 05:43:56 | INFO     | CONFIGURATION (validated)\n2026-02-24 05:43:56 | INFO     | ============================================================\n2026-02-24 05:43:56 | INFO     | Volume paths     : ['/Volumes/accenture/prasun12377/claims_checkpoint_dir', '/Volumes/accenture/prasun12377/bcbsm_volume']\n2026-02-24 05:43:56 | INFO     | Retention days   : 1\n2026-02-24 05:43:56 | INFO     | File extensions  : ('.xlsx', '.xls')\n2026-02-24 05:43:56 | INFO     | Dry run          : False\n2026-02-24 05:43:56 | INFO     | Max retries      : 3\n2026-02-24 05:43:56 | INFO     | Cutoff date (UTC): 2026-02-24 05:38:56\n2026-02-24 05:43:56 | INFO     | ============================================================\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Parse widgets into typed config\n",
    "# ---------------------------------------------------------------------------\n",
    "VOLUME_PATHS = [\n",
    "    p.strip() for p in dbutils.widgets.get(\"volume_paths\").split(\",\") if p.strip()\n",
    "]\n",
    "RETENTION_DAYS = int(dbutils.widgets.get(\"retention_days\"))\n",
    "FILE_EXTENSIONS = tuple(\n",
    "    ext.strip().lower()\n",
    "    for ext in dbutils.widgets.get(\"file_extensions\").split(\",\")\n",
    "    if ext.strip()\n",
    ")\n",
    "DRY_RUN = dbutils.widgets.get(\"dry_run\").lower() == \"true\"\n",
    "MAX_RETRIES = int(dbutils.widgets.get(\"max_retries\"))\n",
    "\n",
    "# Pre-compute the cutoff timestamp (seconds since epoch)\n",
    "#CUTOFF_EPOCH = time.time() - (RETENTION_DAYS * 86400)\n",
    "CUTOFF_EPOCH = time.time() - (5 * 60)\n",
    "CUTOFF_DATE = datetime.fromtimestamp(CUTOFF_EPOCH, tz=timezone.utc)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Input validation - fail fast on misconfiguration\n",
    "# ---------------------------------------------------------------------------\n",
    "_errors = []\n",
    "if not VOLUME_PATHS:\n",
    "    _errors.append(\"volume_paths is empty - at least one path is required\")\n",
    "for _p in VOLUME_PATHS:\n",
    "    if not _p.startswith(\"/Volumes/\"):\n",
    "        _errors.append(f\"Invalid volume path (must start with /Volumes/): {_p}\")\n",
    "if RETENTION_DAYS < 1:\n",
    "    _errors.append(f\"retention_days must be >= 1, got {RETENTION_DAYS}\")\n",
    "if not FILE_EXTENSIONS:\n",
    "    _errors.append(\"file_extensions is empty - at least one extension is required\")\n",
    "for _ext in FILE_EXTENSIONS:\n",
    "    if not _ext.startswith(\".\"):\n",
    "        _errors.append(f\"Extension must start with a dot: '{_ext}'\")\n",
    "if MAX_RETRIES < 0:\n",
    "    _errors.append(f\"max_retries must be >= 0, got {MAX_RETRIES}\")\n",
    "\n",
    "if _errors:\n",
    "    for _e in _errors:\n",
    "        logger.error(f\"VALIDATION FAILURE: {_e}\")\n",
    "    raise ValueError(\n",
    "        f\"Configuration validation failed with {len(_errors)} error(s). \"\n",
    "        \"Check logs above for details.\"\n",
    "    )\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Log validated configuration\n",
    "# ---------------------------------------------------------------------------\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(\"CONFIGURATION (validated)\")\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(f\"Volume paths     : {VOLUME_PATHS}\")\n",
    "logger.info(f\"Retention days   : {RETENTION_DAYS}\")\n",
    "logger.info(f\"File extensions  : {FILE_EXTENSIONS}\")\n",
    "logger.info(f\"Dry run          : {DRY_RUN}\")\n",
    "logger.info(f\"Max retries      : {MAX_RETRIES}\")\n",
    "logger.info(f\"Cutoff date (UTC): {CUTOFF_DATE:%Y-%m-%d %H:%M:%S}\")\n",
    "logger.info(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50c19022-2777-48ea-9191-c28017208be3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f037e7e-efc7-4ef8-9a2c-619fdcad8769",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def format_bytes(size_bytes: int) -> str:\n",
    "    \"\"\"Return a human-readable file-size string.\"\"\"\n",
    "    for unit in (\"B\", \"KB\", \"MB\", \"GB\", \"TB\"):\n",
    "        if abs(size_bytes) < 1024:\n",
    "            return f\"{size_bytes:.1f} {unit}\"\n",
    "        size_bytes /= 1024\n",
    "    return f\"{size_bytes:.1f} PB\"\n",
    "\n",
    "\n",
    "def format_duration(seconds: float) -> str:\n",
    "    \"\"\"Return a human-readable duration string.\"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.1f}s\"\n",
    "    minutes, secs = divmod(seconds, 60)\n",
    "    return f\"{int(minutes)}m {secs:.1f}s\"\n",
    "\n",
    "\n",
    "def scan_files(base_path: str, extensions: tuple[str, ...]) -> list[str]:\n",
    "    \"\"\"Recursively find all files matching the target extensions under *base_path*.\"\"\"\n",
    "    matched = []\n",
    "    try:\n",
    "        for root, _dirs, files in os.walk(base_path):\n",
    "            for fname in files:\n",
    "                if fname.lower().endswith(extensions):\n",
    "                    matched.append(os.path.join(root, fname))\n",
    "    except PermissionError as exc:\n",
    "        logger.warning(f\"Permission denied while scanning {base_path}: {exc}\")\n",
    "    except OSError as exc:\n",
    "        logger.error(f\"OS error scanning {base_path}: {exc}\")\n",
    "    return matched\n",
    "\n",
    "\n",
    "def is_older_than_cutoff(file_path: str, cutoff_epoch: float) -> bool:\n",
    "    \"\"\"Return True if the file's modification time is before the cutoff.\"\"\"\n",
    "    return os.path.getmtime(file_path) < cutoff_epoch\n",
    "\n",
    "\n",
    "def delete_file_with_retry(\n",
    "    file_path: str,\n",
    "    dry_run: bool,\n",
    "    max_retries: int,\n",
    ") -> dict:\n",
    "    \"\"\"Delete a single file with retry logic. Returns a result dict.\n",
    "\n",
    "    Retries on OSError/PermissionError up to *max_retries* times with\n",
    "    exponential back-off (1s, 2s, 4s ...).  PermissionError on all\n",
    "    attempts is treated as a hard error.\n",
    "    \"\"\"\n",
    "    mtime = os.path.getmtime(file_path)\n",
    "    size_bytes = os.path.getsize(file_path)\n",
    "    age_days = (time.time() - mtime) / 86400\n",
    "\n",
    "    result = {\n",
    "        \"file_path\": file_path,\n",
    "        \"size_bytes\": size_bytes,\n",
    "        \"age_days\": round(age_days, 1),\n",
    "        \"modified_utc\": datetime.fromtimestamp(mtime, tz=timezone.utc).strftime(\n",
    "            \"%Y-%m-%d %H:%M:%S\"\n",
    "        ),\n",
    "        \"status\": \"pending\",\n",
    "        \"attempts\": 0,\n",
    "    }\n",
    "\n",
    "    if dry_run:\n",
    "        result[\"status\"] = \"dry_run_skipped\"\n",
    "        return result\n",
    "\n",
    "    # Idempotency guard - file may have been removed between scan and delete\n",
    "    if not os.path.exists(file_path):\n",
    "        result[\"status\"] = \"already_removed\"\n",
    "        logger.info(f\"File already removed (idempotent skip): {file_path}\")\n",
    "        return result\n",
    "\n",
    "    last_exc = None\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        result[\"attempts\"] = attempt\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            # Verify deletion\n",
    "            if not os.path.exists(file_path):\n",
    "                result[\"status\"] = \"deleted\"\n",
    "                return result\n",
    "            else:\n",
    "                logger.warning(\n",
    "                    f\"os.remove returned but file still exists: {file_path}\"\n",
    "                )\n",
    "                result[\"status\"] = \"error: file still exists after os.remove\"\n",
    "                return result\n",
    "        except FileNotFoundError:\n",
    "            # Another process deleted it between our check and os.remove\n",
    "            result[\"status\"] = \"already_removed\"\n",
    "            return result\n",
    "        except (PermissionError, OSError) as exc:\n",
    "            last_exc = exc\n",
    "            if attempt < max_retries:\n",
    "                backoff = 2 ** (attempt - 1)\n",
    "                logger.warning(\n",
    "                    f\"Attempt {attempt}/{max_retries} failed for {file_path}: \"\n",
    "                    f\"{exc} - retrying in {backoff}s\"\n",
    "                )\n",
    "                time.sleep(backoff)\n",
    "\n",
    "    result[\"status\"] = f\"error: {last_exc}\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "909dfa51-3ebc-44b9-a52c-d6a686624348",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Scan and Delete Old Excel Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "107b8f71-27ae-460c-bb53-6ec11c3e55aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-24 05:43:57 | INFO     | Scanning: /Volumes/accenture/prasun12377/claims_checkpoint_dir\n2026-02-24 05:43:57 | INFO     |   Found 1 matching file(s)\n2026-02-24 05:43:57 | INFO     |   [DELETED] /Volumes/accenture/prasun12377/claims_checkpoint_dir/test2/Workforce-Transformation-AD (BOM) v1.xlsx | age=0.0d | size=33.7 KB | modified=2026-02-24 05:33:51\n2026-02-24 05:43:57 | INFO     |   Volume summary: scanned=1 eligible=1 deleted=1 retained=0 errors=0 freed=33.7 KB\n2026-02-24 05:43:57 | INFO     | Scanning: /Volumes/accenture/prasun12377/bcbsm_volume\n2026-02-24 05:43:57 | INFO     |   Found 4 matching file(s)\n2026-02-24 05:43:57 | INFO     |   [DELETED] /Volumes/accenture/prasun12377/bcbsm_volume/test1/DB_Comprehensive_Metrics_TPCC_new.xlsx | age=0.0d | size=148.7 KB | modified=2026-02-24 05:32:25\n2026-02-24 05:43:57 | INFO     |   [DELETED] /Volumes/accenture/prasun12377/bcbsm_volume/test1/ReturnFileActiveMembers_18022026.xls | age=0.0d | size=4.5 KB | modified=2026-02-24 05:32:22\n2026-02-24 05:43:58 | INFO     |   [DELETED] /Volumes/accenture/prasun12377/bcbsm_volume/test1/SQL_GCP_comprehensive_metrics.xlsx | age=0.0d | size=151.6 KB | modified=2026-02-24 05:31:43\n2026-02-24 05:43:58 | INFO     |   [DELETED] /Volumes/accenture/prasun12377/bcbsm_volume/test1/test2/SQL_GCP_comprehensive_metrics.xlsx | age=0.0d | size=151.6 KB | modified=2026-02-24 05:32:40\n2026-02-24 05:43:58 | INFO     |   Volume summary: scanned=4 eligible=4 deleted=4 retained=0 errors=0 freed=456.4 KB\n"
     ]
    }
   ],
   "source": [
    "RUN_START = time.time()\n",
    "\n",
    "all_results: list[dict] = []\n",
    "per_volume_stats: dict[str, dict] = {}  # per-path breakdown\n",
    "\n",
    "global_summary = {\n",
    "    \"paths_configured\": len(VOLUME_PATHS),\n",
    "    \"paths_scanned\": 0,\n",
    "    \"paths_skipped\": 0,\n",
    "    \"files_scanned\": 0,\n",
    "    \"files_eligible\": 0,\n",
    "    \"files_retained\": 0,\n",
    "    \"files_deleted\": 0,\n",
    "    \"files_already_removed\": 0,\n",
    "    \"errors\": 0,\n",
    "    \"total_bytes_freed\": 0,\n",
    "}\n",
    "\n",
    "for vol_path in VOLUME_PATHS:\n",
    "    logger.info(f\"Scanning: {vol_path}\")\n",
    "\n",
    "    vol_stats = {\n",
    "        \"files_scanned\": 0,\n",
    "        \"files_eligible\": 0,\n",
    "        \"files_retained\": 0,\n",
    "        \"files_deleted\": 0,\n",
    "        \"errors\": 0,\n",
    "        \"bytes_freed\": 0,\n",
    "    }\n",
    "\n",
    "    if not os.path.isdir(vol_path):\n",
    "        logger.warning(f\"Path does not exist or is not a directory - skipping: {vol_path}\")\n",
    "        global_summary[\"paths_skipped\"] += 1\n",
    "        per_volume_stats[vol_path] = vol_stats\n",
    "        continue\n",
    "\n",
    "    global_summary[\"paths_scanned\"] += 1\n",
    "    excel_files = scan_files(vol_path, FILE_EXTENSIONS)\n",
    "    vol_stats[\"files_scanned\"] = len(excel_files)\n",
    "    global_summary[\"files_scanned\"] += len(excel_files)\n",
    "    logger.info(f\"  Found {len(excel_files)} matching file(s)\")\n",
    "\n",
    "    for fpath in excel_files:\n",
    "        try:\n",
    "            if not is_older_than_cutoff(fpath, CUTOFF_EPOCH):\n",
    "                vol_stats[\"files_retained\"] += 1\n",
    "                global_summary[\"files_retained\"] += 1\n",
    "                continue\n",
    "        except OSError as exc:\n",
    "            logger.error(f\"  Cannot stat file {fpath}: {exc}\")\n",
    "            vol_stats[\"errors\"] += 1\n",
    "            global_summary[\"errors\"] += 1\n",
    "            all_results.append({\"file_path\": fpath, \"status\": f\"error: {exc}\"})\n",
    "            continue\n",
    "\n",
    "        global_summary[\"files_eligible\"] += 1\n",
    "        vol_stats[\"files_eligible\"] += 1\n",
    "\n",
    "        result = delete_file_with_retry(fpath, DRY_RUN, MAX_RETRIES)\n",
    "        all_results.append(result)\n",
    "\n",
    "        if result[\"status\"] == \"deleted\":\n",
    "            vol_stats[\"files_deleted\"] += 1\n",
    "            vol_stats[\"bytes_freed\"] += result[\"size_bytes\"]\n",
    "            global_summary[\"files_deleted\"] += 1\n",
    "            global_summary[\"total_bytes_freed\"] += result[\"size_bytes\"]\n",
    "        elif result[\"status\"] == \"already_removed\":\n",
    "            global_summary[\"files_already_removed\"] += 1\n",
    "        elif result[\"status\"].startswith(\"error\"):\n",
    "            vol_stats[\"errors\"] += 1\n",
    "            global_summary[\"errors\"] += 1\n",
    "\n",
    "        level = logging.WARNING if result[\"status\"].startswith(\"error\") else logging.INFO\n",
    "        logger.log(\n",
    "            level,\n",
    "            f\"  [{result['status'].upper()}] {result['file_path']} \"\n",
    "            f\"| age={result.get('age_days', '?')}d \"\n",
    "            f\"| size={format_bytes(result.get('size_bytes', 0))} \"\n",
    "            f\"| modified={result.get('modified_utc', 'N/A')}\"\n",
    "            + (f\" | attempts={result.get('attempts')}\" if result.get(\"attempts\", 0) > 1 else \"\"),\n",
    "        )\n",
    "\n",
    "    per_volume_stats[vol_path] = vol_stats\n",
    "    logger.info(\n",
    "        f\"  Volume summary: scanned={vol_stats['files_scanned']} \"\n",
    "        f\"eligible={vol_stats['files_eligible']} \"\n",
    "        f\"deleted={vol_stats['files_deleted']} \"\n",
    "        f\"retained={vol_stats['files_retained']} \"\n",
    "        f\"errors={vol_stats['errors']} \"\n",
    "        f\"freed={format_bytes(vol_stats['bytes_freed'])}\"\n",
    "    )\n",
    "\n",
    "RUN_DURATION = time.time() - RUN_START"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb884d68-ce13-4bf6-813a-de6943988512",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c61b48db-74bc-4e6e-bc3c-cd52bdbad732",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-24 05:43:58 | INFO     | \n2026-02-24 05:43:58 | INFO     | ============================================================\n2026-02-24 05:43:58 | INFO     | EXECUTION SUMMARY\n2026-02-24 05:43:58 | INFO     | ============================================================\n2026-02-24 05:43:58 | INFO     | Mode                 : LIVE DELETE\n2026-02-24 05:43:58 | INFO     | Run duration         : 1.0s\n2026-02-24 05:43:58 | INFO     | Volume paths config  : 2\n2026-02-24 05:43:58 | INFO     | Volume paths scanned : 2\n2026-02-24 05:43:58 | INFO     | Volume paths skipped : 0\n2026-02-24 05:43:58 | INFO     | Total files found    : 5\n2026-02-24 05:43:58 | INFO     | Files retained (<1d): 0\n2026-02-24 05:43:58 | INFO     | Eligible for delete  : 5 (older than 1 days)\n2026-02-24 05:43:58 | INFO     | Files deleted        : 5\n2026-02-24 05:43:58 | INFO     | Already removed      : 0\n2026-02-24 05:43:58 | INFO     | Space freed          : 490.1 KB\n2026-02-24 05:43:58 | INFO     | Errors               : 0\n2026-02-24 05:43:58 | INFO     | ============================================================\n2026-02-24 05:43:58 | INFO     | \n2026-02-24 05:43:58 | INFO     | PER-VOLUME BREAKDOWN\n2026-02-24 05:43:58 | INFO     | ------------------------------------------------------------\n2026-02-24 05:43:58 | INFO     |   /Volumes/accenture/prasun12377/claims_checkpoint_dir: scanned=1 eligible=1 deleted=1 retained=0 errors=0 freed=33.7 KB\n2026-02-24 05:43:58 | INFO     |   /Volumes/accenture/prasun12377/bcbsm_volume: scanned=4 eligible=4 deleted=4 retained=0 errors=0 freed=456.4 KB\n2026-02-24 05:43:58 | INFO     | ------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"\")\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(\"EXECUTION SUMMARY\")\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(f\"Mode                 : {'DRY RUN' if DRY_RUN else 'LIVE DELETE'}\")\n",
    "logger.info(f\"Run duration         : {format_duration(RUN_DURATION)}\")\n",
    "logger.info(f\"Volume paths config  : {global_summary['paths_configured']}\")\n",
    "logger.info(f\"Volume paths scanned : {global_summary['paths_scanned']}\")\n",
    "logger.info(f\"Volume paths skipped : {global_summary['paths_skipped']}\")\n",
    "logger.info(f\"Total files found    : {global_summary['files_scanned']}\")\n",
    "logger.info(f\"Files retained (<{RETENTION_DAYS}d): {global_summary['files_retained']}\")\n",
    "logger.info(f\"Eligible for delete  : {global_summary['files_eligible']} (older than {RETENTION_DAYS} days)\")\n",
    "\n",
    "if DRY_RUN:\n",
    "    logger.info(f\"Files deleted        : 0 (dry run - no files removed)\")\n",
    "else:\n",
    "    logger.info(f\"Files deleted        : {global_summary['files_deleted']}\")\n",
    "    logger.info(f\"Already removed      : {global_summary['files_already_removed']}\")\n",
    "    logger.info(f\"Space freed          : {format_bytes(global_summary['total_bytes_freed'])}\")\n",
    "\n",
    "logger.info(f\"Errors               : {global_summary['errors']}\")\n",
    "logger.info(\"=\" * 60)\n",
    "\n",
    "# Per-volume breakdown\n",
    "logger.info(\"\")\n",
    "logger.info(\"PER-VOLUME BREAKDOWN\")\n",
    "logger.info(\"-\" * 60)\n",
    "for vp, vs in per_volume_stats.items():\n",
    "    logger.info(\n",
    "        f\"  {vp}: scanned={vs['files_scanned']} eligible={vs['files_eligible']} \"\n",
    "        f\"deleted={vs['files_deleted']} retained={vs['files_retained']} \"\n",
    "        f\"errors={vs['errors']} freed={format_bytes(vs['bytes_freed'])}\"\n",
    "    )\n",
    "logger.info(\"-\" * 60)\n",
    "\n",
    "# Error details\n",
    "if global_summary[\"errors\"] > 0:\n",
    "    logger.error(\"\")\n",
    "    logger.error(\"ERROR DETAILS:\")\n",
    "    for r in all_results:\n",
    "        if r[\"status\"].startswith(\"error\"):\n",
    "            logger.error(f\"  {r['file_path']} -> {r['status']}\")\n",
    "\n",
    "if DRY_RUN and global_summary[\"files_eligible\"] > 0:\n",
    "    logger.info(\n",
    "        f\"\\nNOTE: Set dry_run=false to actually delete the \"\n",
    "        f\"{global_summary['files_eligible']} eligible file(s).\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69829e1c-ef0b-4ef8-b287-059d7fdd594b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Exit - Raise on Errors to Fail the Databricks Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ed4a3dd-dbfd-4a1d-ae53-ca0e9e5ed885",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Exit strategy:\n",
    "#   - On ANY errors  -> raise RuntimeError so the Databricks workflow marks\n",
    "#     the run as FAILED, triggering built-in alerting.\n",
    "#   - On success     -> exit cleanly with a status message.\n",
    "# ---------------------------------------------------------------------------\n",
    "mode_label = \"DRY_RUN\" if DRY_RUN else \"LIVE\"\n",
    "\n",
    "exit_msg = (\n",
    "    f\"{mode_label}: \"\n",
    "    f\"scanned={global_summary['files_scanned']} \"\n",
    "    f\"eligible={global_summary['files_eligible']} \"\n",
    "    f\"deleted={global_summary['files_deleted']} \"\n",
    "    f\"errors={global_summary['errors']} \"\n",
    "    f\"freed={format_bytes(global_summary['total_bytes_freed'])} \"\n",
    "    f\"duration={format_duration(RUN_DURATION)}\"\n",
    ")\n",
    "\n",
    "if global_summary[\"errors\"] > 0:\n",
    "    logger.error(f\"Job will FAIL due to {global_summary['errors']} error(s)\")\n",
    "    dbutils.notebook.exit(f\"FAILED: {exit_msg}\")\n",
    "    raise RuntimeError(\n",
    "        f\"Excel cleanup completed with {global_summary['errors']} error(s). \"\n",
    "        f\"Details: {exit_msg}\"\n",
    "    )\n",
    "else:\n",
    "    logger.info(f\"Job completed successfully: {exit_msg}\")\n",
    "    dbutils.notebook.exit(f\"SUCCESS: {exit_msg}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "cleanup_old_excel_files",
   "widgets": {
    "dry_run": {
     "currentValue": "false",
     "nuid": "5780db59-5dbb-4355-a238-3330e941ef3d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "true",
      "label": "Dry run mode (true = list only, false = delete)",
      "name": "dry_run",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "true",
        "false"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "true",
      "label": "Dry run mode (true = list only, false = delete)",
      "name": "dry_run",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "true",
        "false"
       ]
      }
     }
    },
    "file_extensions": {
     "currentValue": ".xlsx,.xls",
     "nuid": "a8997b65-ed53-49ca-b8a3-1f573b1bbcb7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": ".xlsx,.xls",
      "label": "Comma-separated file extensions to target",
      "name": "file_extensions",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": ".xlsx,.xls",
      "label": "Comma-separated file extensions to target",
      "name": "file_extensions",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "max_retries": {
     "currentValue": "3",
     "nuid": "c30b18e6-ca2e-4cea-b0f8-c6bb3467dc02",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "3",
      "label": "Max retry attempts for transient file-deletion failures",
      "name": "max_retries",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "3",
      "label": "Max retry attempts for transient file-deletion failures",
      "name": "max_retries",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "retention_days": {
     "currentValue": "1",
     "nuid": "3716f435-f2a3-495f-bf7c-67d81845b9ad",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "0",
      "label": "Delete files older than this many days",
      "name": "retention_days",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "0",
      "label": "Delete files older than this many days",
      "name": "retention_days",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "volume_paths": {
     "currentValue": "/Volumes/accenture/prasun12377/claims_checkpoint_dir,/Volumes/accenture/prasun12377/bcbsm_volume",
     "nuid": "dfcea1c9-3c7d-4612-84cf-bfa51dabeced",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Volumes/accenture/prasun12377/claims_checkpoint_dir,/Volumes/accenture/prasun12377/bcbsm_volume",
      "label": "Comma-separated volume paths to scan",
      "name": "volume_paths",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Volumes/accenture/prasun12377/claims_checkpoint_dir,/Volumes/accenture/prasun12377/bcbsm_volume",
      "label": "Comma-separated volume paths to scan",
      "name": "volume_paths",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}